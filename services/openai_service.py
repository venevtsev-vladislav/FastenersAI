"""
Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ OpenAI GPT
"""

import logging
import json
from openai import AsyncOpenAI
from config import OPENAI_API_KEY, OPENAI_MODEL
from utils.prompt_loader import PromptLoader

logger = logging.getLogger(__name__)

class OpenAIService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ OpenAI API"""
    
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            default_headers={"OpenAI-Beta": "assistants=v2"}
        )
        self.model = OPENAI_MODEL
        self.assistant_id = "asst_HjGYPyU5L7uTWZA8q4mHUhGA"  # ID Ð²Ð°ÑˆÐµÐ³Ð¾ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°
        self.prompt_loader = PromptLoader()
    
    async def analyze_user_intent(self, text: str) -> dict:
        """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°"""
        try:
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð¸Ð· JSON Ñ„Ð°Ð¹Ð»Ð°
            system_prompt = self.prompt_loader.get_system_prompt("assistant_prompt.json")
            
            user_prompt = f"""ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¸ Ð½Ð°Ð¹Ð´Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÑ€ÐµÐ¿ÐµÐ¶Ð½Ñ‹Ñ… Ð¸Ð·Ð´ÐµÐ»Ð¸ÑÑ….

Ð’ÐÐ–ÐÐž: Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ñ‚ÐµÐºÑÑ‚ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¸Ð· PDF ÑÑ‡ÐµÑ‚Ð°), Ð½Ð°Ð¹Ð´Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ð¼Ð¸/Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑÐ¼Ð¸ Ð¸ Ð¿Ñ€Ð¾Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐ¹ Ñ€ÐµÐºÐ²Ð¸Ð·Ð¸Ñ‚Ñ‹, Ð°Ð´Ñ€ÐµÑÐ°, Ð¸Ñ‚Ð¾Ð³Ð¸ Ð¸ Ð´Ñ€ÑƒÐ³ÑƒÑŽ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ.

Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°:
{text}

Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð¯ ÐŸÐž Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐ˜Ð®:
1. Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ ÑÑ‡ÐµÑ‚/Ð½Ð°ÐºÐ»Ð°Ð´Ð½Ð°Ñ - Ð½Ð°Ð¹Ð´Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ‚Ð¸Ð¿Ð° "â„– ÐÑ€Ñ‚Ð¸ÐºÑƒÐ» Ð¢Ð¾Ð²Ð°Ñ€Ñ‹ ÐšÐ¾Ð»-Ð²Ð¾ Ð•Ð´. Ð¦ÐµÐ½Ð° Ð¡ÑƒÐ¼Ð¼Ð°"
2. Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ - Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð¹ Ð²ÐµÑÑŒ Ñ‚ÐµÐºÑÑ‚
3. Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐ¹ ÑÐ»ÑƒÐ¶ÐµÐ±Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ (Ñ€ÐµÐºÐ²Ð¸Ð·Ð¸Ñ‚Ñ‹, Ð°Ð´Ñ€ÐµÑÐ°, Ð¸Ñ‚Ð¾Ð³Ð¸, Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¸)
4. Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÑ€ÐµÐ¿ÐµÐ¶Ð½Ñ‹Ñ… Ð¸Ð·Ð´ÐµÐ»Ð¸ÑÑ…"""
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=2000  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð·Ð°ÐºÐ°Ð·Ð¾Ð²
            )
            
            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚
            content = response.choices[0].message.content
            try:
                result = json.loads(content)
                logger.info(f"GPT Ð°Ð½Ð°Ð»Ð¸Ð· ÑƒÑÐ¿ÐµÑˆÐµÐ½: {result}")
                return result
            except json.JSONDecodeError:
                logger.warning(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ JSON Ð¾Ñ‚ GPT: {content}")
                # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
                return {
                    "type": "Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾",
                    "diameter": None,
                    "length": None,
                    "material": None,
                    "coating": None,
                    "head_type": None,
                    "standard": None,
                    "quantity": None,
                    "additional_features": [],
                    "confidence": 0.5,
                    "raw_text": text
                }
                
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ Ñ‡ÐµÑ€ÐµÐ· GPT: {e}")
            raise
    
    async def search_query_optimization(self, user_intent: dict) -> str:
        """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° GPT"""
        try:
            system_prompt = """
Ð¢Ñ‹ - ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð¿Ð¾Ð¸ÑÐºÑƒ ÐºÑ€ÐµÐ¿ÐµÐ¶Ð½Ñ‹Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹. ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ ÑÐ¾Ð·Ð´Ð°Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³Ðµ.

Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ð¹:
- ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ (Ñ‚Ð¸Ð¿, Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹)
- Ð¡Ð¸Ð½Ð¾Ð½Ð¸Ð¼Ñ‹ Ð¸ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ
- ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²

Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ, Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².
"""
            
            user_prompt = f"Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ: {json.dumps(user_intent, ensure_ascii=False)}"
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=200
            )
            
            result = response.choices[0].message.content.strip()
            logger.info(f"ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {e}")
            # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
            return f"{user_intent.get('type', 'Ð´ÐµÑ‚Ð°Ð»ÑŒ')} {user_intent.get('diameter', '')} {user_intent.get('length', '')}"

    async def analyze_with_assistant(self, text: str) -> dict:
        """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ñ‡ÐµÑ€ÐµÐ· Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð° Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¼ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰ÐµÐ¼ (Assistants API v2)."""
        try:
            # Import enhanced logging
            try:
                from railway_logging import log_gpt_request, log_gpt_response
            except ImportError:
                def log_gpt_request(text, user_id=None, chat_id=None):
                    logger.info(f"ðŸ¤– GPT Ð—ÐÐŸÐ ÐžÐ¡ | user_id={user_id} | chat_id={chat_id} | text={text[:100]}...")
                
                def log_gpt_response(response, user_id=None, chat_id=None):
                    logger.info(f"âœ… GPT ÐžÐ¢Ð’Ð•Ð¢ | user_id={user_id} | chat_id={chat_id} | response={response}")
            
            # Log GPT request
            log_gpt_request(text)
            logger.info(f"ÐÐ½Ð°Ð»Ð¸Ð· Ñ‡ÐµÑ€ÐµÐ· Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð° Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¼ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰ÐµÐ¼: {text[:100]}...")

            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ thread (Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº v2 Ð·Ð°Ð´Ð°Ð½ Ð½Ð° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ðµ Ñ‡ÐµÑ€ÐµÐ· default_headers)
            thread = await self.client.beta.threads.create()

            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
            await self.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=f"ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÐºÑ€ÐµÐ¿ÐµÐ¶Ð° Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹:\n\n{text}"
            )

            # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°
            run = await self.client.beta.threads.runs.create(
                thread_id=thread.id,
                assistant_id=self.assistant_id
            )

            # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
            import asyncio
            while run.status in ['queued', 'in_progress']:
                await asyncio.sleep(1)
                run = await self.client.beta.threads.runs.retrieve(
                    thread_id=thread.id,
                    run_id=run.id
                )

            if run.status == 'completed':
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚
                messages = await self.client.beta.threads.messages.list(
                    thread_id=thread.id
                )

                assistant_message = messages.data[0].content[0].text.value
                logger.info(f"ÐžÑ‚Ð²ÐµÑ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°: {assistant_message}")

                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON
                try:
                    result = json.loads(assistant_message)
                    logger.info(f"ÐÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ð· ÑƒÑÐ¿ÐµÑˆÐµÐ½: {result}")
                    
                    # Log GPT response with detailed JSON
                    log_gpt_response(result)
                    
                    return result
                except json.JSONDecodeError as json_error:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð¾Ñ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°: {json_error}")
                    logger.error(f"ÐžÑ‚Ð²ÐµÑ‚ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°: {assistant_message}")
                    return {'type': 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾', 'confidence': 0.1}
            else:
                logger.error(f"ÐÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»ÑÑ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹: {run.status}")
                return {'type': 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾', 'confidence': 0.1}

        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð¾Ð¼: {e}")
            return {'type': 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾', 'confidence': 0.1}
    
    async def call_gpt(self, prompt: str, model: str = "gpt-4o-mini", 
                      temperature: float = 0.1, max_tokens: int = 500) -> str:
        """Generic GPT call method for validation and other tasks"""
        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Error calling GPT: {e}")
            return f"Error: {str(e)}"

