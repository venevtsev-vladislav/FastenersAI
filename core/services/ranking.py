"""
–ï–¥–∏–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É –∏–∑ Python –∏ Deno Edge Function
"""

import logging
import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from shared.logging import get_logger

logger = get_logger(__name__)


@dataclass
class MatchAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞"""
    type_match: bool
    standard_match: bool
    size_match: bool
    coating_match: bool
    matched_tokens: List[str]
    explanation: List[str]


@dataclass
class RankingTokens:
    """–¢–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
    type_tok: str | None
    std_toks: List[str]
    mxl_toks: List[str]
    coat_toks: List[str]


class RankingService:
    """–°–µ—Ä–≤–∏—Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        self.weights = {
            'type': 25,        # 25% –∑–∞ —Ç–∏–ø –¥–µ—Ç–∞–ª–∏
            'standard': 40,    # 40% –∑–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç DIN
            'size': 30,        # 30% –∑–∞ —Ç–æ—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            'coating': 15      # 15% –∑–∞ –ø–æ–∫—Ä—ã—Ç–∏–µ
        }
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        self.combination_bonuses = {
            'standard_size': 15,      # –°—Ç–∞–Ω–¥–∞—Ä—Ç + —Ä–∞–∑–º–µ—Ä—ã
            'type_size': 10,          # –¢–∏–ø + —Ä–∞–∑–º–µ—Ä—ã
            'full_match': 20          # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        }
    
    def normalize_string(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not text:
            return ""
        
        return text.lower().strip() \
            .replace('√ó', 'x') \
            .replace('—Ö', 'x') \
            .replace(' ', '') \
            .replace('din', 'din') \
            .replace('–æ—Ü–∏–Ω–∫', '—Ü–∏–Ω–∫')
    
    def canonicalize_diameter(self, diameter: str) -> str:
        """–ö–∞–Ω–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∏–∞–º–µ—Ç—Ä"""
        if not diameter:
            return ""
        
        return diameter.replace(' –º–º', '').upper().replace('–ú', 'M')
    
    def canonicalize_length(self, length: str) -> str:
        """–ö–∞–Ω–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–ª–∏–Ω—É"""
        if not length:
            return ""
        
        return length.replace(' –º–º', '')
    
    def generate_mxl_variants(self, diameter: str, length: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–º–µ—Ä–æ–≤ MxL"""
        D = self.canonicalize_diameter(diameter)
        L = self.canonicalize_length(length)
        
        if not D or not L:
            return []
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–≥–æ–ª–∫–æ–≤ (—Ñ–æ—Ä–º–∞—Ç 50x50x40)
        if 'x' in D and not D.startswith('M'):
            # –≠—Ç–æ —É–≥–æ–ª–æ–∫, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            xs = ["x", "—Ö", "√ó"]
            variants = []
            for X in xs:
                variants.extend([
                    D.replace('x', X),
                    D.replace('—Ö', X),
                    D.replace('√ó', X)
                ])
            return list(set(variants))
        
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—Ç–æ–≤/–≤–∏–Ω—Ç–æ–≤
        d_lat = D
        d_cyr = D.replace('M', '–ú')
        xs = ["x", "—Ö", "√ó", "-"]
        variants = []
        
        for X in xs:
            variants.extend([
                f"{d_lat}{X}{L}",
                f"{d_lat} {X} {L}",
                f"{d_cyr}{X}{L}",
                f"{d_cyr} {X} {L}"
            ])
        
        variants.extend([f"{d_lat}{L}", f"{d_cyr}{L}"])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –µ—Å–ª–∏ –¥–∏–∞–º–µ—Ç—Ä –¥–µ—Å—è—Ç–∏—á–Ω—ã–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        decimal_match = re.match(r'^(\d+)[\.,](\d+)$', D)
        if decimal_match:
            fractional = decimal_match.group(2)
            alt_tokens = [
                f"{fractional}x{L}",
                f"{fractional} x {L}",
                f"{fractional}—Ö{L}",
                f"{fractional} —Ö {L}",
                f"{fractional}√ó{L}",
                f"{fractional} √ó {L}"
            ]
            variants.extend(alt_tokens)
        
        return list(set(variants))
    
    def extract_tokens(self, query: str, user_intent: Dict[str, Any]) -> RankingTokens:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        type_tok = user_intent.get('type')
        std_toks = []
        mxl_toks = []
        coat_toks = []
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã
        if user_intent.get('standard'):
            std_toks.append(user_intent['standard'])
        
        # –†–∞–∑–º–µ—Ä—ã
        diameter = user_intent.get('diameter')
        length = user_intent.get('length')
        if diameter and length:
            mxl_variants = self.generate_mxl_variants(diameter, length)
            mxl_toks.extend(mxl_variants)
        
        # –ü–æ–∫—Ä—ã—Ç–∏—è
        if user_intent.get('coating'):
            coat_toks.append(user_intent['coating'])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_lower = query.lower()
        if 'din' in query_lower:
            din_match = re.search(r'din\s*(\d+)', query_lower)
            if din_match:
                std_toks.append(f"DIN{din_match.group(1)}")
        
        return RankingTokens(
            type_tok=type_tok,
            std_toks=list(set(std_toks)),
            mxl_toks=list(set(mxl_toks)),
            coat_toks=list(set(coat_toks))
        )
    
    def analyze_matches(self, name: str, tokens: RankingTokens) -> MatchAnalysis:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è —Å —Ç–æ–∫–µ–Ω–∞–º–∏"""
        normalized_name = self.normalize_string(name)
        
        analysis = MatchAnalysis(
            type_match=False,
            standard_match=False,
            size_match=False,
            coating_match=False,
            matched_tokens=[],
            explanation=[]
        )
        
        logger.debug(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º '{name}' —Å —Ç–æ–∫–µ–Ω–∞–º–∏: {tokens}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞
        if tokens.type_tok and self.normalize_string(tokens.type_tok) in normalized_name:
            analysis.type_match = True
            analysis.matched_tokens.append(tokens.type_tok)
            analysis.explanation.append(f"‚úÖ –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞: '{tokens.type_tok}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞
        matched_standards = [
            std for std in tokens.std_toks 
            if self.normalize_string(std) in normalized_name
        ]
        if matched_standards:
            analysis.standard_match = True
            analysis.matched_tokens.extend(matched_standards)
            analysis.explanation.append(f"‚úÖ –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞: {', '.join(matched_standards)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
        matched_sizes = [
            size for size in tokens.mxl_toks 
            if self.normalize_string(size) in normalized_name
        ]
        if matched_sizes:
            analysis.size_match = True
            analysis.matched_tokens.extend(matched_sizes)
            analysis.explanation.append(f"‚úÖ –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: {', '.join(matched_sizes)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è
        matched_coatings = [
            coat for coat in tokens.coat_toks 
            if self.normalize_string(coat) in normalized_name
        ]
        if matched_coatings:
            analysis.coating_match = True
            analysis.matched_tokens.extend(matched_coatings)
            analysis.explanation.append(f"‚úÖ –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è: {', '.join(matched_coatings)}")
        
        return analysis
    
    def calculate_probability(self, analysis: MatchAnalysis) -> Tuple[int, str]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        total_score = 0
        explanation = analysis.explanation.copy()
        
        # –ë–∞–∑–æ–≤—ã–µ –æ—á–∫–∏ –∑–∞ –∫–∞–∂–¥—ã–π —Ç–∏–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if analysis.type_match:
            total_score += self.weights['type']
            explanation.append(f"üìä –í–∫–ª–∞–¥ —Ç–∏–ø–∞: +{self.weights['type']}%")
        
        if analysis.standard_match:
            total_score += self.weights['standard']
            explanation.append(f"üìä –í–∫–ª–∞–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞: +{self.weights['standard']}%")
        
        if analysis.size_match:
            total_score += self.weights['size']
            explanation.append(f"üìä –í–∫–ª–∞–¥ —Ä–∞–∑–º–µ—Ä–æ–≤: +{self.weights['size']}%")
        
        if analysis.coating_match:
            total_score += self.weights['coating']
            explanation.append(f"üìä –í–∫–ª–∞–¥ –ø–æ–∫—Ä—ã—Ç–∏—è: +{self.weights['coating']}%")
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        if analysis.standard_match and analysis.size_match:
            bonus = self.combination_bonuses['standard_size']
            total_score += bonus
            explanation.append(f"üéØ –ë–æ–Ω—É—Å –∑–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç + —Ä–∞–∑–º–µ—Ä—ã: +{bonus}%")
        
        if analysis.type_match and analysis.size_match:
            bonus = self.combination_bonuses['type_size']
            total_score += bonus
            explanation.append(f"üéØ –ë–æ–Ω—É—Å –∑–∞ —Ç–∏–ø + —Ä–∞–∑–º–µ—Ä—ã: +{bonus}%")
        
        if analysis.type_match and analysis.standard_match and analysis.size_match:
            bonus = self.combination_bonuses['full_match']
            total_score += bonus
            explanation.append(f"üéØ –ë–æ–Ω—É—Å –∑–∞ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: +{bonus}%")
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
        if analysis.standard_match and analysis.size_match:
            # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–ª–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –∏ —Ä–∞–∑–º–µ—Ä—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–µ –º–µ–Ω—å—à–µ 80%
            total_score = max(total_score, 80)
            if total_score > 80:
                explanation.append("üîí –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç+—Ä–∞–∑–º–µ—Ä—ã: 80%")
        
        elif analysis.type_match and not analysis.size_match and not analysis.standard_match:
            # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–ª —Ç–æ–ª—å–∫–æ —Ç–∏–ø –±–µ–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ ‚Äî –Ω–µ –±–æ–ª–µ–µ 40%
            total_score = min(total_score, 40)
            explanation.append("üîí –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ–ª—å–∫–æ —Ç–∏–ø–∞: 40%")
        
        elif analysis.coating_match and not analysis.type_match and not analysis.size_match and not analysis.standard_match:
            # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–ª–æ —Ç–æ–ª—å–∫–æ –ø–æ–∫—Ä—ã—Ç–∏–µ ‚Äî –Ω–µ –±–æ–ª–µ–µ 20%
            total_score = min(total_score, 20)
            explanation.append("üîí –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ–ª—å–∫–æ –ø–æ–∫—Ä—ã—Ç–∏—è: 20%")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-100%
        probability = max(0, min(100, round(total_score)))
        explanation.append(f"üìà –ò—Ç–æ–≥–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability}%")
        
        return probability, '\n'.join(explanation)
    
    def get_match_reason(self, name: str, tokens: RankingTokens) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏—á–∏–Ω—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        normalized_name = self.normalize_string(name)
        
        if tokens.type_tok and self.normalize_string(tokens.type_tok) in normalized_name and \
           any(self.normalize_string(t) in normalized_name for t in tokens.mxl_toks):
            return '–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏ —Ä–∞–∑–º–µ—Ä–æ–≤'
        
        if tokens.type_tok and self.normalize_string(tokens.type_tok) in normalized_name:
            return '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–µ—Ç–∞–ª–∏'
        
        if any(self.normalize_string(t) in normalized_name for t in tokens.std_toks):
            return '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞'
        
        if any(self.normalize_string(t) in normalized_name for t in tokens.mxl_toks):
            return '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤'
        
        if any(self.normalize_string(t) in normalized_name for t in tokens.coat_toks):
            return '–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è'
        
        return '–ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é'
    
    def rank_results(self, results: List[Dict[str, Any]], query: str, user_intent: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        if not results:
            return results
        
        logger.info(f"–†–∞–Ω–∂–∏—Ä—É—é {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        tokens = self.extract_tokens(query, user_intent)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        for result in results:
            analysis = self.analyze_matches(result.get('name', ''), tokens)
            probability, explanation = self.calculate_probability(analysis)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            result.update({
                'relevance_score': probability,
                'probability_percent': probability,
                'confidence_score': probability / 100.0,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                'match_reason': self.get_match_reason(result.get('name', ''), tokens),
                'explanation': explanation,
                'matched_tokens': analysis.matched_tokens,
                'search_query': query,
                'full_query': query
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
        results.sort(key=lambda x: x.get('probability_percent', 0), reverse=True)
        
        logger.info(f"–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –¢–æ–ø-3:")
        for i, result in enumerate(results[:3]):
            logger.info(f"  {i+1}. {result.get('sku', 'N/A')}: {result.get('probability_percent', 0)}% - {result.get('match_reason', 'N/A')}")
        
        return results
